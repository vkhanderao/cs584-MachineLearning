{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Multi-class Classification and Neural Network\n",
    "> **FULL MARKS = 10**\n",
    "\n",
    "In this assignment, you are going to implement your own neural network to do multi-class classification. We use one-vs-all strategy here by training multiple binary classifiers (one for each class).\n",
    "\n",
    "Please notice that you can only use numpy and scipy.optimize.minimize. **No** library versions of other method are allowed.  . Follow the instructions, you will need to fill the blanks to make it functional. The process is similar to the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y):\n",
    "    idx = np.arange(len(X))\n",
    "    train_size = int(len(X) * 2/3)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(num_in, num_out):\n",
    "    '''\n",
    "    :param num_in: the number of input units in the weight array\n",
    "    :param num_out: the number of output units in the weight array\n",
    "    '''\n",
    "\n",
    "    # Note that 'W' contains both weights and bias, you can consider W[0, :] as bias\n",
    "    W = np.zeros((1 + num_in, num_out))\n",
    "    \n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement Xavier/Glorot uniform initialization                                  #\n",
    "    #                                                                                 #\n",
    "    # Hint: you can find the reference here:                                          #\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform  #\n",
    "    ###################################################################################\n",
    "    r1=np.sqrt(6 / (num_in + num_out))\n",
    "    for i in range(num_in):\n",
    "        for j in range(num_out):\n",
    "            W[i][j]=np.float32(np.random.uniform(-r1, r1))\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "    ###################################################################################\n",
    "    # Full Mark: 0.5                                                                  #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement sigmoid function:                                                     #\n",
    "    #                             sigmoid(x) = 1/(1+e^(-x))                           #\n",
    "    ###################################################################################\n",
    "\n",
    "    res = 1/ (1 + np.exp(-x))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "    ###################################################################################\n",
    "    # Full Mark: 0.5                                                                  #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement tanh function:                                                        #\n",
    "    #                     tanh(x) = (e^x-e^(-x)) / (e^x+e^(-x))                       #\n",
    "    ###################################################################################\n",
    "\n",
    "    res = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Computes the gradient of the sigmoid function evaluated at x.                   #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "    sig=sigmoid(x)\n",
    "    grad = sig*(1-sig)\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_gradient(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Computes the gradient of the tanh function evaluated at x.                      #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "    tan= tanh(x)\n",
    "    grad = (1 - np.square(tan))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, X):\n",
    "    '''\n",
    "    :param W: weights (including biases) of the neural network. It is a list containing both W_hidden and W_output.\n",
    "    :param X: input. Already added one additional column of all \"1\"s.\n",
    "    '''\n",
    "\n",
    "    # Shape of W_hidden: [num_feature+1, num_hidden]\n",
    "    # Shape pf W_output: [num_hidden+1, num_output]\n",
    "    W_hidden, W_output = W\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement the forward pass. You need to compute four values:                    #\n",
    "    # z_hidden, a_hidden, z_output, a_output                                          #\n",
    "    #                                                                                 #\n",
    "    # Note that our neural network consists of three layers:                          #\n",
    "    # Input -> Hidden -> Output                                                       #\n",
    "    # The activation function in hidden layer is 'tanh'                               #\n",
    "    # The activation function in output layer is 'sigmoid'                            #\n",
    "    ###################################################################################\n",
    "\n",
    "    z_hidden = np.matmul(np.squeeze(np.array(X)),np.squeeze(np.array(W_hidden)))\n",
    "    a_hidden=tanh(z_hidden)\n",
    "    final_a = np.concatenate((np.ones((a_hidden.shape[0],1)),np.array(a_hidden)),axis = 1)\n",
    "    z_output = np.matmul(np.squeeze(np.array(final_a)),np.squeeze(np.array(W_output)))\n",
    "    a_output=sigmoid(z_output)\n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    # z_hidden is the raw output of hidden layer, a_hidden is the result after performing activation on z_hidden\n",
    "    # z_output is the raw output of output layer, a_output is the result after performing activation on z_output\n",
    "    return {'z_hidden': z_hidden, 'a_hidden': a_hidden,\n",
    "            'z_output': z_output, 'a_output': a_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_funtion(W, X, y, num_feature, num_hidden, num_output, L2_lambda):\n",
    "    '''\n",
    "    :param W: a 1D array containing all weights and biases.\n",
    "    :param X: input\n",
    "    :param y: labels of X\n",
    "    :param num_feature: number of features in X\n",
    "    :param num_hidden: number of hidden units\n",
    "    :param num_output: number of output units\n",
    "    :param L2_lambda: the coefficient of regularization term\n",
    "    '''\n",
    "\n",
    "    m = y.size\n",
    "\n",
    "    # Reshape W back into the parameters W_hidden and W_output\n",
    "    W_hidden = np.reshape(W[:num_hidden * (num_feature + 1)],\n",
    "                          ((num_feature + 1), num_hidden))\n",
    "\n",
    "    W_output = np.reshape(W[(num_hidden * (num_feature + 1)):],\n",
    "                          ((num_hidden + 1), num_output))\n",
    "\n",
    "    # Initialize grads\n",
    "    W_hidden_grad = np.zeros(W_hidden.shape)\n",
    "    W_output_grad = np.zeros(W_output.shape)\n",
    "\n",
    "    # Add one column of \"1\"s to X\n",
    "    X_input = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    ##########################################################################################\n",
    "    # Full Mark: 3                                                                           #\n",
    "    # TODO:                                                                                  #\n",
    "    # 1. Transform y to one-hot encoding. Implement binary cross-entropy loss function       #\n",
    "    # (Hint: Use the forward function to get necessary outputs from the model)               #\n",
    "    #                                                                                        #\n",
    "    # 2. Add L2 regularization to all weights in loss                                        #\n",
    "    # (Note that we will not add regularization to bias)                                     #\n",
    "    #                                                                                        #\n",
    "    # 3. Compute the gradient for W_hidden and W_output (including both weights and biases)  #\n",
    "    # (Hint: use chain rule, and the sigmoid gradient, tanh gradient you have                #\n",
    "    # implemented above. Don't forget to add the gradient of regularization term)            #\n",
    "    ##########################################################################################\n",
    "    \n",
    "    #One-Hot Encoding\n",
    "    final_y = np.eye(3)[y]\n",
    "    f_out = forward([W_hidden_grad,W_output_grad],X_input)\n",
    "    L = -np.sum(np.log(f_out['a_output'][np.where(final_y == 1)]))\n",
    "    \n",
    "    W_output_grad =np.matmul(f_out['a_output'],np.matmul(f_out['z_output'].T,(f_out['a_output']-final_y)))\n",
    "    W_hidden_grad = np.matmul(f_out['a_hidden'],np.matmul(W_output_grad.T,f_out['z_hidden']).T)\n",
    "    \n",
    "#     ###################################################################################\n",
    "#     #                       END OF YOUR CODE                                          #\n",
    "#     ###################################################################################\n",
    "\n",
    "    grads = np.concatenate([W_hidden_grad.ravel(), W_output_grad.ravel()])\n",
    "\n",
    "    return L,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(initial_W, X, y, num_epoch, num_feature, num_hidden, num_output, L2_lambda):\n",
    "    '''\n",
    "    :param initial_W: initial weights as a 1D array.\n",
    "    :param X: input\n",
    "    :param y: labels of X\n",
    "    :param num_epoch: number of iterations\n",
    "    :param num_feature: number of features in X\n",
    "    :param num_hidden: number of hidden units\n",
    "    :param num_output: number of output units\n",
    "    :param L2_lambda: the coefficient of regularization term\n",
    "    '''\n",
    "\n",
    "    options = {'maxiter': num_epoch}\n",
    "\n",
    "    ###########################################################################################\n",
    "    # Full Mark: 1                                                                            #\n",
    "    # TODO:                                                                                   #\n",
    "    # Optimize weights                                                                        #\n",
    "    # (Hint: use scipy.optimize.minimize to automatically do the iteration.)                  #\n",
    "    # ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) #\n",
    "    # For some optimizers, you need to set 'jac' as True.                                     #\n",
    "    # You may need to use lambda to create a function with one parameter to wrap the          #\n",
    "    # loss_funtion you have implemented above.                                                #\n",
    "    #                                                                                         #\n",
    "    # Note that scipy.optimize.minimize only accepts a 1D weight array as initial weights,    #\n",
    "    # and the output optimized weights will also be a 1D array.                               #\n",
    "    # That is why we unroll the initial weights into a single long vector.                    #\n",
    "    ###########################################################################################\n",
    "\n",
    "    W_final = \n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    # Obtain W_hidden and W_output back from W_final\n",
    "    W_hidden = np.reshape(W_final[:num_hidden * (num_feature + 1)],\n",
    "                          ((num_feature + 1), num_hidden))\n",
    "    W_output = np.reshape(W_final[(num_hidden * (num_feature + 1)):],\n",
    "                          ((num_hidden + 1), num_output))\n",
    "\n",
    "    return [W_hidden, W_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, y_test, W):\n",
    "    '''\n",
    "    :param X_test: input\n",
    "    :param y_test: labels of X_test\n",
    "    :param W: a list containing two weights W_hidden and W_output.\n",
    "    '''\n",
    "\n",
    "    test_input = np.concatenate([np.ones((y_test.size, 1)), X_test], axis=1)\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Predict on test set and compute the accuracy.                                   #\n",
    "    # (Hint: use forward function to get predicted outpt)                            #\n",
    "    #        u                                                                         #\n",
    "    ###################################################################################\n",
    "    \n",
    "    f_out = forward([W[0],W[1]],test_input)\n",
    "    pred = np.argmax(f_out['a_output'],axis = 1)\n",
    "    acc = (y_test == pred).sum() / y_test.shape[0]\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this part #\n",
    "# Define parameters\n",
    "NUM_FEATURE = 4\n",
    "NUM_HIDDEN_UNIT = 10\n",
    "NUM_OUTPUT_UNIT = 3\n",
    "NUM_EPOCH = 100\n",
    "L2_lambda = 1\n",
    "\n",
    "# Load data\n",
    "X, y = load_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Initialize weights\n",
    "initial_W_hidden = init_weights(NUM_FEATURE, NUM_HIDDEN_UNIT)\n",
    "initial_W_output = init_weights(NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT)\n",
    "initial_W = np.concatenate([initial_W_hidden.ravel(), initial_W_output.ravel()], axis=0)\n",
    "\n",
    "# # Neural network learning\n",
    "# W = optimize(initial_W, X_train, y_train, NUM_EPOCH, NUM_FEATURE, NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT, L2_lambda)\n",
    "\n",
    "# # Predict\n",
    "# acc = predict(X_test, y_test, W)\n",
    "# print(\"Test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
