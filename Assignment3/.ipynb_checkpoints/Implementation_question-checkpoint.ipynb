{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S3RIrtBdsozG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Txu9oJV6sxAp"
   },
   "outputs": [],
   "source": [
    "class mulNode():\n",
    "  def forward(self,x,y):\n",
    "    self.x = np.squeeze(np.array(x))\n",
    "    self.y = np.squeeze(np.array(y))\n",
    "    z = np.matmul(self.x,self.y)\n",
    "    return z\n",
    "  def backward(self,dz):\n",
    "    dx = dz.dot(self.y.T)\n",
    "    dy = self.x.T.dot(dz)\n",
    "    return [dx,dy]\n",
    "\n",
    "class addNode():\n",
    "  def forward(self,x,y):\n",
    "    # print(x.shape,y.shape)\n",
    "    self.x = np.squeeze(np.array(x))\n",
    "    self.y = np.squeeze(np.array(y))\n",
    "    z = np.add(self.x,self.y)\n",
    "    return z\n",
    "  def backward(self,dz):\n",
    "    dx = 1*dz\n",
    "    dy = 1*dz\n",
    "    return [dx,dy]\n",
    "\n",
    "class sigmoidNode():\n",
    "  def forward(self,x):\n",
    "    self.x = x\n",
    "    self.h = 1/(1+np.exp(-x))\n",
    "    return self.h\n",
    "  def backward(self,dz):\n",
    "    dx = dz*self.h*(1-self.h)\n",
    "    return dx\n",
    "  \n",
    "\n",
    "class softmaxNode():\n",
    "  def forward(self,x):\n",
    "    self.x = x\n",
    "    self.z = np.exp(self.x).T/(np.sum(np.exp(self.x),axis = 1))\n",
    "    return self.z.T\n",
    "  def backward(self,dldh):\n",
    "      dldx = []\n",
    "      h = self.z.T\n",
    "      for i in range(len(h)): #For each of the m training examples\n",
    "          s = h[i]\n",
    "          softmax = s.reshape(-1,1)\n",
    "          #Create a 3x3 softmax derivative matrix\n",
    "          d = np.diagflat(s) - np.dot(softmax,softmax.T)\n",
    "          #dldh * dhdx for this instance\n",
    "          dldx.append(dldh[i].dot(d))\n",
    "      dldx = np.array(dldx) #a mx3 matrix of derivative\n",
    "      return dldx \n",
    "\n",
    "class categoricalLossNode():\n",
    "  def forward(self,y,y_hat):\n",
    "    self.y = y\n",
    "    self.y_hat = y_hat\n",
    "    z = -np.sum(np.log(y_hat[np.where(y == 1)]))\n",
    "    return z\n",
    "  def backward(self):\n",
    "    dz = 1* (self.y_hat - self.y )\n",
    "    return dz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3IL32l6uGUuQ"
   },
   "outputs": [],
   "source": [
    "class computation_graph():\n",
    "  def __init__(self,x,y,hidden):\n",
    "    # self.x  = x\n",
    "    # self.y = y\n",
    "    self.hidden = hidden\n",
    "    self.l = categoricalLossNode()\n",
    "    self.softmax = softmaxNode()\n",
    "    self.w1 = np.ones((x.shape[1],hidden))*0.01\n",
    "    self.b1 = np.ones((1,hidden))*0.01\n",
    "    self.multiply1  = mulNode()\n",
    "    self.addition1 = addNode()\n",
    "    self.sigmoid1  = sigmoidNode()\n",
    "\n",
    "    self.w2 = np.ones((hidden,hidden))*0.01\n",
    "    self.b2 = np.ones((1,hidden))*0.01\n",
    "    self.multiply2  = mulNode()\n",
    "    self.addition2 = addNode()\n",
    "    self.sigmoid2  = sigmoidNode()\n",
    "\n",
    "    self.w3 = np.ones((hidden,y.shape[1]))*0.01\n",
    "    self.b3 = np.ones((1,y.shape[1]))*0.01\n",
    "    self.multiply3  = mulNode()\n",
    "    self.addition3 = addNode()\n",
    "  \n",
    "  def forward(self,x,y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    m = self.multiply1.forward(self.x,self.w1)\n",
    "    add = self.addition1.forward(m,self.b1)\n",
    "    first_out = self.sigmoid1.forward(add)\n",
    "\n",
    "    m_s = self.multiply2.forward(first_out,self.w2)\n",
    "    add_s = self.addition2.forward(m_s,self.b2)\n",
    "    second_out = self.sigmoid2.forward(add_s)\n",
    "\n",
    "    m_l = self.multiply3.forward(second_out,self.w3)\n",
    "    add_l = self.addition3.forward(m_l,self.b3)\n",
    "    pred = self.softmax.forward(add_l)\n",
    "\n",
    "    loss = self.l.forward(self.y,pred)\n",
    "\n",
    "    return loss,pred\n",
    "  \n",
    "  def backward(self):\n",
    "    self.bias = []\n",
    "    self.grad = []\n",
    "    loss_back = self.l.backward()\n",
    "    ds = self.softmax.backward(loss_back)\n",
    "    bias_last = self.addition3.backward(ds)\n",
    "    grad_last = self.multiply3.backward(bias_last[0])\n",
    "    self.bias.append(bias_last[0])\n",
    "    self.grad.append(grad_last[1])\n",
    "\n",
    "    sig = self.sigmoid2.backward(grad_last[0])\n",
    "    bias_hs = self.addition2.backward(sig)\n",
    "    grad_hs = self.multiply2.backward(bias_hs[0])\n",
    "    self.bias.append(bias_hs[0])\n",
    "    self.grad.append(grad_hs[1])\n",
    "\n",
    "    sig = self.sigmoid1.backward(grad_hs[0])\n",
    "    bias_hf = self.addition1.backward(sig)\n",
    "    grad_hf = self.multiply1.backward(bias_hf[0])\n",
    "    self.bias.append(bias_hf[0])\n",
    "    self.grad.append(grad_hf[1])\n",
    "\n",
    "    return self.bias,self.grad\n",
    "\n",
    "  def next_batch(self,x,y,batch_size):\n",
    "    #returns the data splits equal to batch\n",
    "    for i in range(0,x.shape[0],batch_size):\n",
    "      yield (x[i:i+batch_size],y[i:i+batch_size])\n",
    "\n",
    "  def fit(self,x,y,epochs,learning_rate = 0.01,eta = 0.01,batch_size = 5):\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "    self.batch_size = batch_size\n",
    "    i = 1\n",
    "    it = epochs\n",
    "    eps0 = learning_rate\n",
    "    eps_final = eps0/100\n",
    "    decay_rates = [((1-(i+1)/it)*eps0 + ((i+1)/it)*eps_final) for i in range(it)]\n",
    "    for i in range(epochs):\n",
    "      batch_loss = []\n",
    "      batch_accuracy = []\n",
    "      learning_rate = decay_rates[i]\n",
    "      for batch_x,batch_y in self.next_batch(x,y,self.batch_size):\n",
    "          l,y_hat = cg.forward(batch_x,batch_y)\n",
    "          bias,grad = cg.backward()\n",
    "\n",
    "          batch_loss.append(l)\n",
    "          batch_accuracy.append(accuracy_score(np.argmax(batch_y,axis=1),np.argmax(y_hat,axis = 1)))\n",
    "\n",
    "          #updating bias\n",
    "          self.b1 = self.b1 - learning_rate*bias[2]\n",
    "          self.b2 = self.b2 - learning_rate*bias[1]\n",
    "          self.b3 = self.b3 - learning_rate*bias[0]\n",
    "\n",
    "          #updating weights \n",
    "          self.w1 = self.w1 - learning_rate*grad[2]\n",
    "          self.w2 = self.w2 - learning_rate*grad[1]\n",
    "          self.w3 = self.w3 - learning_rate*grad[0]\n",
    "     \n",
    "      epoch_loss.append(np.mean(batch_loss))\n",
    "      epoch_accuracy.append(np.mean(batch_accuracy))\n",
    "      print(\"Epoch :{} Loss :{} accuracy :{}\".format(i,epoch_loss[i],epoch_accuracy[i]))\n",
    "\n",
    "  def evaluate(self,x,y):\n",
    "    batch_loss = []\n",
    "    batch_accuracy = []\n",
    "    for batch_x,batch_y in self.next_batch(x,y,self.batch_size):\n",
    "      loss,y_hat = cg.forward(batch_x,batch_y)\n",
    "      accuracy = accuracy_score(np.argmax(batch_y,axis=1),np.argmax(y_hat,axis = 1))\n",
    "      batch_loss.append(loss)\n",
    "      batch_accuracy.append(accuracy)\n",
    "    print(\"loss:{} accuracy:{}\".format(np.mean(batch_loss),np.mean(batch_accuracy)))\n",
    "  \n",
    "  def get_values(self):\n",
    "    return (self.w1,self.w2,self.w3,self.b1,self.b2,self.b3)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "57zyC_WU3f7o"
   },
   "outputs": [],
   "source": [
    "def load_iris_data():\n",
    "  cols = ['sepal_length','sepal_width','petal_length','petal_width','class']\n",
    "  iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None)\n",
    "  iris_df.columns = cols\n",
    "  x_train,x_test,y_train,y_test = train_test_split(iris_df.iloc[:,:-1],iris_df['class'],test_size = 0.30,random_state = 10)\n",
    "\n",
    "  return x_train,x_test,to_categorical(np.asarray(y_train.factorize()[0])),to_categorical(np.asarray(y_test.factorize()[0]))\n",
    "\n",
    "def load_spam_data():\n",
    "  cols = ['word_freq_make','word_freq_address','word_freq_all','word_freq_3d','word_freq_our','word_freq_over','word_freq_remove',\n",
    "  'word_freq_internet','word_freq_order','word_freq_mail','word_freq_receive','word_freq_will','word_freq_people','word_freq_report',\n",
    "  'word_freq_addresses','word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your',\n",
    "  'word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george','word_freq_650','word_freq_lab',\n",
    "  'word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data','word_freq_415','word_freq_85','word_freq_technology',\n",
    "  'word_freq_1999','word_freq_parts','word_freq_pm','word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original',\n",
    "  'word_freq_project','word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(','char_freq_[',\n",
    "  'char_freq_!','char_freq_$','char_freq_#','capital_run_length_average','capital_run_length_longest','capital_run_length_total','class']\n",
    "  spam_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',header=None)\n",
    "  spam_df.columns = cols\n",
    "  x_train,x_test,y_train,y_test = train_test_split(spam_df.iloc[:,:-1],spam_df['class'],test_size = 0.30,random_state = 10)\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  sc = StandardScaler()\n",
    "  x_train = sc.fit_transform(x_train)\n",
    "  x_test = sc.transform(x_test)\n",
    "\n",
    "  return x_train,x_test[1:],to_categorical(y_train),to_categorical(y_test)[1:]\n",
    "\n",
    "#Cifar Data\n",
    "def extract_indices(sub_classes,y):\n",
    "  ind = []\n",
    "  for i in sub_classes:\n",
    "    ind.append((y == i))\n",
    "  return ind\n",
    "random.seed(10)\n",
    "sub_classes = random.sample(range(0,9),3)\n",
    "def load_cifar_data():\n",
    "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "  indices = extract_indices(sub_classes,y_train)\n",
    "  sub_x_train = np.append(np.append(x_train[indices[0].reshape(x_train.shape[0])],(x_train[indices[1].reshape(x_train.shape[0])]),axis=0),x_train[indices[0].reshape(x_train.shape[0])],axis = 0)\n",
    "  sub_y_train = np.append(np.append(y_train[indices[0]],y_train[indices[1]],axis=0),y_train[indices[2]],axis = 0)\n",
    "  indices = extract_indices(sub_classes,y_test)\n",
    "  sub_x_test = np.append(np.append(x_test[indices[0].reshape(x_test.shape[0])],(x_test[indices[1].reshape(x_test.shape[0])]),axis=0),x_test[indices[0].reshape(x_test.shape[0])],axis = 0)\n",
    "  sub_y_test = np.append(np.append(y_test[indices[0]],y_test[indices[1]],axis=0),y_test[indices[2]],axis = 0)\n",
    "\n",
    "  x_train = sub_x_train/255\n",
    "  x_test = sub_x_test/255\n",
    "  y_train = to_categorical(sub_y_train,num_classes=10)\n",
    "  y_test = to_categorical(sub_y_test,num_classes=10)\n",
    "  return (x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R2J6S2ge4ADU"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = load_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcvs3GJbIaqO",
    "outputId": "f82915e8-2e96-4203-eedd-f528796bce05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0 Loss :5.499128750548144 accuracy :0.2857142857142858\n",
      "Epoch :1 Loss :5.495734811651935 accuracy :0.3428571428571429\n",
      "Epoch :2 Loss :5.493051938916548 accuracy :0.35238095238095246\n",
      "Epoch :3 Loss :5.49081574995295 accuracy :0.36190476190476195\n",
      "Epoch :4 Loss :5.4888931037224165 accuracy :0.38095238095238093\n",
      "Epoch :5 Loss :5.487214192298916 accuracy :0.39047619047619053\n",
      "Epoch :6 Loss :5.485741158365049 accuracy :0.4000000000000001\n",
      "Epoch :7 Loss :5.484452962185308 accuracy :0.4000000000000001\n",
      "Epoch :8 Loss :5.483337827900071 accuracy :0.41904761904761917\n",
      "Epoch :9 Loss :5.482389384909998 accuracy :0.4571428571428572\n",
      "loss:5.4845968138636545 accuracy:0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "cg = computation_graph(x_train,y_train,64)\n",
    "cg.fit(x_train,y_train,10,learning_rate=0.001)\n",
    "cg.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3wDzeQN70Yl2"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = load_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFLy_LJb5X3a",
    "outputId": "32a10c35-d6d5-4b91-9445-97f944fedf3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0 Loss :3.5076189209689095 accuracy :0.5795031055900621\n",
      "Epoch :1 Loss :3.4861751441017246 accuracy :0.584472049689441\n",
      "Epoch :2 Loss :3.4657666911315226 accuracy :0.584472049689441\n",
      "Epoch :3 Loss :3.446377914273318 accuracy :0.584472049689441\n",
      "Epoch :4 Loss :3.427928042569083 accuracy :0.5894409937888199\n",
      "Epoch :5 Loss :3.410309863505084 accuracy :0.5975155279503105\n",
      "Epoch :6 Loss :3.393393120078283 accuracy :0.6043478260869565\n",
      "Epoch :7 Loss :3.3770499652236596 accuracy :0.6090062111801242\n",
      "Epoch :8 Loss :3.3612140882601578 accuracy :0.6108695652173912\n",
      "Epoch :9 Loss :3.345288716698243 accuracy :0.6108695652173912\n",
      "loss:3.381793463270455 accuracy:0.594927536231884\n"
     ]
    }
   ],
   "source": [
    "cg = computation_graph(x_train,y_train,512)\n",
    "cg.fit(x_train,y_train,10,learning_rate=0.001)\n",
    "cg.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6O2rswZ84NA",
    "outputId": "37a943c7-0d95-496c-e147-6ce8abde768f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = load_cifar_data()\n",
    "x_train = np.reshape(x_train,(x_train.shape[0],3072))\n",
    "x_test = np.reshape(x_test,(x_test.shape[0],3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sIWLmLS-WAy",
    "outputId": "66a15e9d-97b3-427b-d735-6dfa8eb1ac39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0 Loss :0.7942177801425812 accuracy :0.9753333333333334\n",
      "Epoch :1 Loss :1.0759865301182425 accuracy :0.961\n",
      "Epoch :2 Loss :1.1874192408746134 accuracy :0.9536666666666667\n",
      "Epoch :3 Loss :1.5621869219943079 accuracy :0.9333333333333333\n",
      "Epoch :4 Loss :9.134605116700627 accuracy :0.5\n",
      "loss:10.03025123391114 accuracy:0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "cg = computation_graph(x_train,y_train,512)\n",
    "cg.fit(x_train,y_train,5,learning_rate=0.001)\n",
    "cg.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FN_D4tzAVfc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Implementation_question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
